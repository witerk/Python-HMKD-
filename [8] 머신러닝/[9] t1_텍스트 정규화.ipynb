{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e302fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random, re, time, xgboost\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "#맷플랏립 그래프 주피터에서 바로 출력하기\n",
    "from matplotlib import rc\n",
    "rc('font', family='Malgun Gothic')      #한글 폰트설정\n",
    "plt.rcParams['axes.unicode_minus']=False      #마이너스 부호 출력 설정\n",
    "\n",
    "from scipy import stats, sparse\n",
    "from scipy.stats import t, norm, chi2, chi2_contingency, skew\n",
    "from sklearn.datasets import load_iris, make_classification, load_breast_cancer, load_boston, make_blobs\n",
    "from sklearn import datasets, tree\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, Binarizer, PolynomialFeatures\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, \\\n",
    "    recall_score, confusion_matrix, f1_score, classification_report,\\\n",
    "    precision_recall_curve, roc_auc_score, mean_squared_error, r2_score,\\\n",
    "    mean_absolute_error, silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, \\\n",
    "    GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import plot_importance, XGBClassifier, XGBRegressor\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260753a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893b163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33d79f6",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d0185b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sample = '''The Matrix is everywhere its all around us, here even in this room. \n",
    "               You can see it out your window or on your television. \n",
    "               You feel it when you go to work, or go to church or pay your taxes.'''\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "\n",
    "print(type(sentences), len(sentences))\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184d87d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2417139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sent = sent_tokenize(text)\n",
    "    word_tokn = [word_tokenize(i) for i in sent]\n",
    "    return word_tokn\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2886ea3b",
   "metadata": {},
   "source": [
    "### stopwords(불용어) 제거\n",
    ": 일반적이고 의미없는 단어들의 집합 (문맥 이해에 도움이 안 되거나 문서간의 차이 구분에 기여하지 않는 단어들/ the, is, in, and 등)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f86c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'영어 stop words 갯수: {len(nltk.corpus.stopwords.words(\"english\"))}')\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa33420e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = nltk.corpus.stopwords.words('english')  #불용어 목록\n",
    "all_tokens = []\n",
    "\n",
    "for s in word_tokens:   #문장별로 단어들 모두 분리\n",
    "    filtered_words = []\n",
    "    for word in s:\n",
    "        word = word.lower()   #각 단어 소문자로 변환\n",
    "        if word not in stopwords_list:\n",
    "            filtered_words.append(word)   #불용어가 아니면 리스트에 추가\n",
    "    all_tokens.append(filtered_words)  #불용어가 아닌 단어들로 이루어진 문장 리스트에 추가\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8836784",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca284fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()   #활용 단어들의 어원을 찾아줌\n",
    "\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf73e387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bd20a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'), lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'), lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))\n",
    "# 단어, 품사를 넣어줌 -> v(동사), a(형용사)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cbc22",
   "metadata": {},
   "source": [
    "### 희소행렬\n",
    ": 대부분의 요소가 0인 행렬<br>\n",
    "큰 행렬을 효율적으로 저장하고 처리하기 위해 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0055f30",
   "metadata": {},
   "source": [
    "### 희소 행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7356b585",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = np.array([[3,0,1],[0,2,0]])\n",
    "dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b264d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0이 아닌 데이터 추출 (3,1,2)\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 (0,0),(0,2),(1,0)\n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용해 coo형식 희소행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))\n",
    "# 데이터, 위치정보 -> 2중 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220f6345",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sparse_coo)\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "dense2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e47c34f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 0이 아닌 데이터 추출, 위치값\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "row = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# coo형식으로 희소행렬 생성\n",
    "sparse1 = sparse.coo_matrix((data2, (row, col)))\n",
    "print(sparse1)\n",
    "sparse1.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e898db",
   "metadata": {},
   "source": [
    "### 희소 행렬 - CSR 형식\n",
    ": coo보다 조금 더 효율적으로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb6f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0이 아닌 데이터 추출, 위치값\n",
    "data2 = np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
    "row = np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
    "col = np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
    "\n",
    "# row배열에서 값이 바뀌는 위치\n",
    "# 마지막 13은 row행의 길이(개수)\n",
    "row_col_ind = np.array([0,2,7,9,10,12,13])\n",
    "\n",
    "# CSR 형식으로 변환\n",
    "saprse2 = sparse.csr_matrix((data2, col, row_col_ind))\n",
    "print(saprse2)\n",
    "saprse2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8587b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d54a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e234ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d43ec24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750431d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78b6348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80855cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c03739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a37d0152",
   "metadata": {},
   "source": [
    "## 8.2 텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36511728",
   "metadata": {},
   "source": [
    "### Text Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec22dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
    "               You can see it out your window or on your television. \\\n",
    "               You feel it when you go to work, or go to church or pay your taxes.'\n",
    "sentences = sent_tokenize(text=text_sample)\n",
    "print(type(sentences),len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d0c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
    "words = word_tokenize(sentence)\n",
    "print(type(words), len(words))\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#여러개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화 만드는 함수 생성\n",
    "def tokenize_text(text):\n",
    "    \n",
    "    # 문장별로 분리 토큰\n",
    "    sentences = sent_tokenize(text)\n",
    "    # 분리된 문장별 단어 토큰화\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "#여러 문장들에 대해 문장별 단어 토큰화 수행. \n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(type(word_tokens),len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d31069",
   "metadata": {},
   "source": [
    "### Stopwords 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a0aa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd78d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
    "print(nltk.corpus.stopwords.words('english')[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efe780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "# 위 예제의 3개의 문장별로 얻은 word_tokens list 에 대해 stop word 제거 Loop\n",
    "for sentence in word_tokens:\n",
    "    filtered_words=[]\n",
    "    # 개별 문장별로 tokenize된 sentence list에 대해 stop word 제거 Loop\n",
    "    for word in sentence:\n",
    "        #소문자로 모두 변환합니다. \n",
    "        word = word.lower()\n",
    "        # tokenize 된 개별 word가 stop words 들의 단어에 포함되지 않으면 word_tokens에 추가\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "    \n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1a3c24",
   "metadata": {},
   "source": [
    "### Stemming과 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c8c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
    "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
    "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03150f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'),lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('happier','a'),lemma.lemmatize('happiest','a'))\n",
    "print(lemma.lemmatize('fancier','a'),lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d3974",
   "metadata": {},
   "source": [
    "## 8.3 Bag of Words – BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3171e",
   "metadata": {},
   "source": [
    "### 희소 행렬 - COO 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dense = np.array( [ [ 3, 0, 1 ], [0, 2, 0 ] ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1ad68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0,0,1])\n",
    "col_pos = np.array([0,2,1])\n",
    "\n",
    "# sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos,col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b5c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9925de6f",
   "metadata": {},
   "source": [
    "### 희소 행렬 – CSR 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99888c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "# 0 이 아닌 데이터 추출\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "# 행 위치와 열 위치를 각각 array로 생성 \n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO 형식으로 변환 \n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR 형식으로 변환 \n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
    "print(sparse_csr.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064a3319",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense3 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "coo = sparse.coo_matrix(dense3)\n",
    "csr = sparse.csr_matrix(dense3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed0b8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34c7138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
