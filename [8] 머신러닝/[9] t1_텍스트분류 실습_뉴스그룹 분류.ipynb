{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baf72a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random, re, time, xgboost\n",
    "import numpy as np\n",
    "from numpy.linalg import svd\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "#맷플랏립 그래프 주피터에서 바로 출력하기\n",
    "from matplotlib import rc\n",
    "rc('font', family='Malgun Gothic')      #한글 폰트설정\n",
    "plt.rcParams['axes.unicode_minus']=False      #마이너스 부호 출력 설정\n",
    "\n",
    "from scipy import stats, sparse\n",
    "from scipy.stats import t, norm, chi2, chi2_contingency, skew\n",
    "from sklearn.datasets import load_iris, make_classification, load_breast_cancer, load_boston, make_blobs\n",
    "from sklearn import datasets, tree\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, Binarizer, PolynomialFeatures\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, \\\n",
    "    recall_score, confusion_matrix, f1_score, classification_report,\\\n",
    "    precision_recall_curve, roc_auc_score, mean_squared_error, r2_score,\\\n",
    "    mean_absolute_error, silhouette_samples, silhouette_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, \\\n",
    "    GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import plot_importance, XGBClassifier, XGBRegressor\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.pipeline import Pipeline\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c73e3fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46561de9",
   "metadata": {},
   "source": [
    "### 텍스트 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f927932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "news_data = fetch_20newsgroups(subset='all', random_state=156)\n",
    "\n",
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "003c996c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target 클래스의 값과 분포도: \n",
      "0     799\n",
      "1     973\n",
      "2     985\n",
      "3     982\n",
      "4     963\n",
      "5     988\n",
      "6     975\n",
      "7     990\n",
      "8     996\n",
      "9     994\n",
      "10    999\n",
      "11    991\n",
      "12    984\n",
      "13    990\n",
      "14    987\n",
      "15    997\n",
      "16    910\n",
      "17    940\n",
      "18    775\n",
      "19    628\n",
      "dtype: int64\n",
      "\n",
      "target 클래스의 이름들 \n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(f'target 클래스의 값과 분포도: \\n{pd.Series(news_data.target).value_counts().sort_index()}')\n",
    "print(f'\\ntarget 클래스의 이름들 \\n{news_data.target_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82288d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: egreen@east.sun.com (Ed Green - Pixel Cruncher)\n",
      "Subject: Re: Observation re: helmets\n",
      "Organization: Sun Microsystems, RTP, NC\n",
      "Lines: 21\n",
      "Distribution: world\n",
      "Reply-To: egreen@east.sun.com\n",
      "NNTP-Posting-Host: laser.east.sun.com\n",
      "\n",
      "In article 211353@mavenry.altcit.eskimo.com, maven@mavenry.altcit.eskimo.com (Norman Hamer) writes:\n",
      "> \n",
      "> The question for the day is re: passenger helmets, if you don't know for \n",
      ">certain who's gonna ride with you (like say you meet them at a .... church \n",
      ">meeting, yeah, that's the ticket)... What are some guidelines? Should I just \n",
      ">pick up another shoei in my size to have a backup helmet (XL), or should I \n",
      ">maybe get an inexpensive one of a smaller size to accomodate my likely \n",
      ">passenger? \n",
      "\n",
      "If your primary concern is protecting the passenger in the event of a\n",
      "crash, have him or her fitted for a helmet that is their size.  If your\n",
      "primary concern is complying with stupid helmet laws, carry a real big\n",
      "spare (you can put a big or small head in a big helmet, but not in a\n",
      "small one).\n",
      "\n",
      "---\n",
      "Ed Green, former Ninjaite |I was drinking last night with a biker,\n",
      "  Ed.Green@East.Sun.COM   |and I showed him a picture of you.  I said,\n",
      "DoD #0111  (919)460-8302  |\"Go on, get to know her, you'll like her!\"\n",
      " (The Grateful Dead) -->  |It seemed like the least I could do...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b64f56dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "학습 데이터 크기: 11314, 테스트 데이터 크기: 7532\n"
     ]
    }
   ],
   "source": [
    "# subset='train'으로 학습용 데이터만 추출\n",
    "# remove=('headers','footers','quotes')로 내용만 추출\n",
    "train_news = fetch_20newsgroups(subset='train', remove=('headers','footers','quotes'), random_state=156)\n",
    "xtrain = train_news.data\n",
    "ytrain = train_news.target\n",
    "print(type(xtrain))\n",
    "\n",
    "test_news = fetch_20newsgroups(subset='test', remove=('headers','footers','quotes'), random_state=156)\n",
    "xtest = test_news.data\n",
    "ytest = test_news.target\n",
    "print(f'학습 데이터 크기: {len(train_news.data)}, 테스트 데이터 크기: {len(test_news.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d16d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "What I did NOT get with my drive (CD300i) is the System Install CD you\n",
      "listed as #1.  Any ideas about how I can get one?  I bought my IIvx 8/120\n",
      "from Direct Express in Chicago (no complaints at all -- good price & good\n",
      "service).\n",
      "\n",
      "BTW, I've heard that the System Install CD can be used to boot the mac;\n",
      "however, my drive will NOT accept a CD caddy is the machine is off.  How can\n",
      "you boot with it then?\n",
      "\n",
      "--Dave\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_news.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc8439",
   "metadata": {},
   "source": [
    "### 피처 백터화 변환 & 머신러닝 모델 학습,예측,평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25ea6e7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2791073463.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\h\\AppData\\Local\\Temp\\ipykernel_11468\\2791073463.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    텍스트 데이터를 특징 벡터로 변환하기 위해 사용\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# 텍스트 데이터를 특징 벡터로 변환하기 위해 사용\n",
    "# 단어빈도(Count) or TF-IDF값(단어빈도+희소성)을 기반으로 특징벡터 생성\n",
    "\n",
    "ct_vect = CountVectorizer()\n",
    "ct_vect.fit(xtrain)\n",
    "xtrain_ct_vect = ct_vect.transform(xtrain)\n",
    "\n",
    "xtest_ct_vect = ct_vect.transform(xtest)\n",
    "\n",
    "print(f'학습데이터 test의 countvectorizer shape: {xtrain_ct_vect.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70bbac06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Logistic Regression의 예측정확도는 0.6076739245884227\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(xtrain_ct_vect, ytrain)\n",
    "\n",
    "pred = lr_clf.predict(xtest_ct_vect)\n",
    "acc = accuracy_score(ytest, pred)\n",
    "print(f'CountVectorizer Logistic Regression의 예측정확도는 {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4325ab84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Logistic Regression 의 예측 정확도는 0.674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(xtrain)\n",
    "\n",
    "xtrain_tfidf_vect = tfidf_vect.transform(xtrain)\n",
    "xtest_tfidf_vect = tfidf_vect.transform(xtest)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(xtrain_tfidf_vect , ytrain)\n",
    "\n",
    "pred = lr_clf.predict(xtest_tfidf_vect)\n",
    "acc = accuracy_score(ytest, pred)\n",
    "print(f'TF-IDF Logistic Regression 의 예측 정확도는 {acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9e0c269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorized Logistic Regression 의 예측 정확도는 0.692\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)\n",
    "tfidf_vect.fit(xtrain)\n",
    "\n",
    "xtrain_tfidf_vect = tfidf_vect.transform(xtrain)\n",
    "xtest_tfidf_vect = tfidf_vect.transform(xtest)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(xtrain_tfidf_vect, ytrain)\n",
    "\n",
    "pred = lr_clf.predict(xtest_tfidf_vect)\n",
    "acc = accuracy_score(ytest, pred)\n",
    "print(f'TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f54db2",
   "metadata": {},
   "source": [
    "#### 여기부터 러닝타임 엄청 오래 걸림"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69638fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Logistic Regression best C parameter: {'C': 10}\n",
      "TF-IDF Logistic Regression 의 예측 정확도는 0.701\n"
     ]
    }
   ],
   "source": [
    "# C: 알파의 역수(작을수록 규제 강도가 큼)\n",
    "params = {'C':[0.01, 0.1, 1, 5, 10]}\n",
    "grid_lr = GridSearchCV(lr_clf, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "grid_lr.fit(xtrain_tfidf_vect, ytrain)\n",
    "print(f'Logistic Regression best C parameter: {grid_lr.best_params_}')\n",
    "\n",
    "# 최적 C값으로 학습된 gird_cv로 예측 수행하고 정확도 평가\n",
    "pred = grid_lr.predict(xtest_tfidf_vect)\n",
    "acc = accuracy_score(ytest, pred)\n",
    "print(f'TF-IDF Logistic Regression 의 예측 정확도는 {acc:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043095f",
   "metadata": {},
   "source": [
    "#### 파이프라인부터는 아래 코드 참고(오래걸려서 돌려보지x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aed0e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ec104",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c41853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e372f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca58e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14e3d032",
   "metadata": {},
   "source": [
    "### 텍스트 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf2fd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news_data = fetch_20newsgroups(subset='all',random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b66de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57169781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print('target 클래스의 값과 분포도 \\n',pd.Series(news_data.target).value_counts().sort_index())\n",
    "print('target 클래스의 이름들 \\n',news_data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55526b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(news_data.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2512b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# subset='train'으로 학습용(Train) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "train_news= fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), random_state=156)\n",
    "X_train = train_news.data\n",
    "y_train = train_news.target\n",
    "print(type(X_train))\n",
    "\n",
    "# subset='test'으로 테스트(Test) 데이터만 추출, remove=('headers', 'footers', 'quotes')로 내용만 추출\n",
    "test_news= fetch_20newsgroups(subset='test',remove=('headers', 'footers','quotes'),random_state=156)\n",
    "X_test = test_news.data\n",
    "y_test = test_news.target\n",
    "print('학습 데이터 크기 {0} , 테스트 데이터 크기 {1}'.format(len(train_news.data) , len(test_news.data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84e7498",
   "metadata": {},
   "source": [
    "### 피처 벡터화 변환과 머신러닝 모델 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e802abde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Count Vectorization으로 feature extraction 변환 수행. \n",
    "cnt_vect = CountVectorizer()\n",
    "\n",
    "cnt_vect.fit(X_train)\n",
    "X_train_cnt_vect = cnt_vect.transform(X_train)\n",
    "\n",
    "# 학습 데이터로 fit( )된 CountVectorizer를 이용하여 테스트 데이터를 feature extraction 변환 수행. \n",
    "X_test_cnt_vect = cnt_vect.transform(X_test)\n",
    "\n",
    "print('학습 데이터 Text의 CountVectorizer Shape:',X_train_cnt_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5cb6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_cnt_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_cnt_vect)\n",
    "print('CountVectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test,pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa45a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorization 적용하여 학습 데이터셋과 테스트 데이터 셋 변환. \n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "# LogisticRegression을 이용하여 학습/예측/평가 수행. \n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c47261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop words 필터링을 추가하고 ngram을 기본(1,1)에서 (1,2)로 변경하여 Feature Vectorization 적용.\n",
    "tfidf_vect = TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300 )\n",
    "tfidf_vect.fit(X_train)\n",
    "X_train_tfidf_vect = tfidf_vect.transform(X_train)\n",
    "X_test_tfidf_vect = tfidf_vect.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(X_train_tfidf_vect , y_train)\n",
    "pred = lr_clf.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b721fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 최적 C 값 도출 튜닝 수행. CV는 3 Fold셋으로 설정. \n",
    "params = { 'C':[0.01, 0.1, 1, 5, 10]}\n",
    "grid_cv_lr = GridSearchCV(lr_clf ,param_grid=params , cv=3 , scoring='accuracy' , verbose=1 )\n",
    "grid_cv_lr.fit(X_train_tfidf_vect , y_train)\n",
    "print('Logistic Regression best C parameter :',grid_cv_lr.best_params_ )\n",
    "\n",
    "# 최적 C 값으로 학습된 grid_cv로 예측 수행하고 정확도 평가. \n",
    "pred = grid_cv_lr.predict(X_test_tfidf_vect)\n",
    "print('TF-IDF Vectorized Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4365fd",
   "metadata": {},
   "source": [
    "### 사이킷런 파이프라인(Pipeline) 사용 및 GridSearchCV와의 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb3276",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# TfidfVectorizer 객체를 tfidf_vect 객체명으로, LogisticRegression객체를 lr_clf 객체명으로 생성하는 Pipeline생성\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2), max_df=300)),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear', C=10))\n",
    "])\n",
    "\n",
    "# 별도의 TfidfVectorizer객체의 fit_transform( )과 LogisticRegression의 fit(), predict( )가 필요 없음. \n",
    "# pipeline의 fit( ) 과 predict( ) 만으로 한꺼번에 Feature Vectorization과 ML 학습/예측이 가능. \n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc3f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english')),\n",
    "    ('lr_clf', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# Pipeline에 기술된 각각의 객체 변수에 언더바(_)2개를 연달아 붙여 GridSearchCV에 사용될 \n",
    "# 파라미터/하이퍼 파라미터 이름과 값을 설정. . \n",
    "params = { 'tfidf_vect__ngram_range': [(1,1), (1,2), (1,3)],\n",
    "           'tfidf_vect__max_df': [100, 300, 700],\n",
    "           'lr_clf__C': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# GridSearchCV의 생성자에 Estimator가 아닌 Pipeline 객체 입력\n",
    "grid_cv_pipe = GridSearchCV(pipeline, param_grid=params, cv=3 , scoring='accuracy',verbose=1)\n",
    "grid_cv_pipe.fit(X_train , y_train)\n",
    "print(grid_cv_pipe.best_params_ , grid_cv_pipe.best_score_)\n",
    "\n",
    "pred = grid_cv_pipe.predict(X_test)\n",
    "print('Pipeline을 통한 Logistic Regression 의 예측 정확도는 {0:.3f}'.format(accuracy_score(y_test ,pred)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07deba19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef929e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
