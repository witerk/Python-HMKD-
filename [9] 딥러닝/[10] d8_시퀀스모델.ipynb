{"cells":[{"cell_type":"markdown","metadata":{"id":"5Xd_jva3JvIF"},"source":["### 단어를 시퀀스로 처리하기: 시퀀스 모델 방식"]},{"cell_type":"markdown","metadata":{"id":"gJDW7N7JJvIG"},"source":["#### 첫 번째 예제"]},{"cell_type":"markdown","metadata":{"id":"oBcqxVV3JvIG"},"source":["**데이터 다운로드**"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vM4VpJ7KJvIH","outputId":"5419edb1-4477-447f-f76b-d1bd374a4c90","executionInfo":{"status":"ok","timestamp":1689300564601,"user_tz":-540,"elapsed":6964,"user":{"displayName":"김나영","userId":"16985821314079982465"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["rm: cannot remove 'aclImdb': No such file or directory\n","  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100 80.2M  100 80.2M    0     0  20.9M      0  0:00:03  0:00:03 --:--:-- 20.9M\n"]}],"source":["!rm -r aclImdb\n","!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n","!tar -xf aclImdb_v1.tar.gz\n","!rm -r aclImdb/train/unsup"]},{"cell_type":"markdown","metadata":{"id":"ztDHYG4gJvII"},"source":["**데이터 준비**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"azorKPW0JvII","outputId":"3c6f2ef2-f49b-4929-f028-d5447b913f5c","executionInfo":{"status":"ok","timestamp":1689300573760,"user_tz":-540,"elapsed":9159,"user":{"displayName":"김나영","userId":"16985821314079982465"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 20000 files belonging to 2 classes.\n","Found 5000 files belonging to 2 classes.\n","Found 25000 files belonging to 2 classes.\n"]}],"source":["import os, pathlib, shutil, random\n","from tensorflow import keras\n","batch_size = 32\n","base_dir = pathlib.Path(\"aclImdb\")\n","val_dir = base_dir / \"val\"\n","train_dir = base_dir / \"train\"\n","\n","for category in (\"neg\", \"pos\"):\n","    os.makedirs(val_dir / category)\n","    files = os.listdir(train_dir / category)\n","    random.Random(1337).shuffle(files)\n","    num_val_samples = int(0.2 * len(files))\n","    val_files = files[-num_val_samples:]\n","    for fname in val_files:\n","        shutil.move(train_dir / category / fname,\n","                    val_dir / category / fname)\n","\n","train_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/train\", batch_size=batch_size\n",")\n","val_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/val\", batch_size=batch_size\n",")\n","test_ds = keras.utils.text_dataset_from_directory(\n","    \"aclImdb/test\", batch_size=batch_size\n",")\n","text_only_train_ds = train_ds.map(lambda x, y: x)"]},{"cell_type":"markdown","metadata":{"id":"Zz4tz-n4JvIJ"},"source":["**정수 시퀀스 데이터셋 준비하기**"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"iuPqw9cqJvIJ","executionInfo":{"status":"ok","timestamp":1689300576997,"user_tz":-540,"elapsed":3238,"user":{"displayName":"김나영","userId":"16985821314079982465"}}},"outputs":[],"source":["# 텍스트 데이터를 벡터화하여 신경망 모델에 입력하기 위한 전처리 과정\n","from tensorflow.keras import layers\n","\n","max_length = 600    #시퀀스의 최대 길이\n","max_tokens = 20000   #한 데이터에서 사용되는 단어들의 최대 개수(많이 쓰이는 거 위주로 2만개만 쓰겠다)\n","\n","text_vectorization = layers.TextVectorization(   #텍스트 벡터화 레이어\n","    max_tokens=max_tokens,\n","    output_mode=\"int\",   #텍스트 데이터를 정수로 변환\n","    output_sequence_length=max_length,)\n","text_vectorization.adapt(text_only_train_ds)   #벡터화 레이어를 데이터셋에 적용\n","\n","#벡터화된 정수 시퀀스를 데이터셋에 매핑\n","int_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)\n","int_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls=4)"]},{"cell_type":"markdown","metadata":{"id":"7rCBxfAFJvIK"},"source":["**원-핫 인코딩된 벡터 시퀀스로 시퀀스 모델 만들기**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"id":"3rai-1TDJvIK","outputId":"3fd39524-e9f9-45ee-ec42-bcf37bfedfa4","executionInfo":{"status":"error","timestamp":1689303675013,"user_tz":-540,"elapsed":557,"user":{"displayName":"김나영","userId":"16985821314079982465"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-382857075707>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#텍스트 시퀀스를 입력으로 받음( 각 단어를 정수로 표현)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_tokens\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#원핫 인코딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#양방향 LSTM 추가(입력 시퀀스를 앞뒤로 모두 고려)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"]}],"source":["import tensorflow as tf\n","\n","inputs = keras.Input(shape=(None,), dtype=\"int64\")   #텍스트 시퀀스를 입력으로 받음( 각 단어를 정수로 표현)\n","embedded = tf.one_hot(inputs, depth=max_tokens)   #원핫 인코딩\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)   #양방향 LSTM 추가(입력 시퀀스를 앞뒤로 모두 고려)\n","x = layers.Dropout(0.5)(x)   #드롭아웃 레이어\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)   #출력 레이어\n","\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"TeqpjAQoJvIL"},"source":["**첫 번째 시퀀스 모델 훈련하기**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bl6MtP3NJvIL","outputId":"872b590d-73d5-4bbc-f016-b933426992b4","executionInfo":{"status":"ok","timestamp":1689302261775,"user_tz":-540,"elapsed":1684418,"user":{"displayName":"김나영","userId":"16985821314079982465"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 160s 242ms/step - loss: 0.5645 - accuracy: 0.7035 - val_loss: 0.3933 - val_accuracy: 0.8464\n","Epoch 2/10\n","625/625 [==============================] - 154s 246ms/step - loss: 0.3659 - accuracy: 0.8630 - val_loss: 0.3681 - val_accuracy: 0.8488\n","Epoch 3/10\n","625/625 [==============================] - 153s 246ms/step - loss: 0.2987 - accuracy: 0.8946 - val_loss: 0.3377 - val_accuracy: 0.8634\n","Epoch 4/10\n","625/625 [==============================] - 154s 246ms/step - loss: 0.2504 - accuracy: 0.9121 - val_loss: 0.3014 - val_accuracy: 0.8790\n","Epoch 5/10\n","625/625 [==============================] - 153s 245ms/step - loss: 0.2178 - accuracy: 0.9248 - val_loss: 0.3090 - val_accuracy: 0.8834\n","Epoch 6/10\n","625/625 [==============================] - 153s 245ms/step - loss: 0.1949 - accuracy: 0.9338 - val_loss: 0.3360 - val_accuracy: 0.8844\n","Epoch 7/10\n","625/625 [==============================] - 153s 245ms/step - loss: 0.1739 - accuracy: 0.9434 - val_loss: 0.3296 - val_accuracy: 0.8854\n","Epoch 8/10\n","625/625 [==============================] - 153s 246ms/step - loss: 0.1488 - accuracy: 0.9521 - val_loss: 0.4397 - val_accuracy: 0.8800\n","Epoch 9/10\n","625/625 [==============================] - 154s 246ms/step - loss: 0.1307 - accuracy: 0.9595 - val_loss: 0.3728 - val_accuracy: 0.8688\n","Epoch 10/10\n","625/625 [==============================] - 153s 245ms/step - loss: 0.1075 - accuracy: 0.9660 - val_loss: 0.3849 - val_accuracy: 0.8688\n","782/782 [==============================] - 94s 118ms/step - loss: 0.3000 - accuracy: 0.8761\n","테스트 정확도: 0.876\n"]}],"source":["callbacks = [ keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n","                                    save_best_only=True)]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n","\n","print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"cVY_SwScJvIL"},"source":["#### 단어 임베딩 이해하기"]},{"cell_type":"markdown","metadata":{"id":"visueffQJvIL"},"source":["#### 임베딩 층으로 단어 임베딩 학습하기"]},{"cell_type":"markdown","metadata":{"id":"rH2EhPHUJvIM"},"source":["**`Embedding` 층 만들기**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNACHsDAJvIM"},"outputs":[],"source":["# 임베딩 레이어: 입력 정수 시퀀스 최대값, 임베딩 벡터 크기(차원)을 매개변수로 받음\n","embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)\n","\n","# 텍스트 데이터(고차원 희소벡터)의 각 단어를 고정된 크기의 실수 벡터(저차원 밀집벡터)로 변환\n","# 단어표현 학습/ 차원축소/ 단어간 유사성(거리, 유사도) 비교/ 전이학습(학습된 모델 사용해 초기 가중치 설정)"]},{"cell_type":"markdown","metadata":{"id":"MfBr2Ld8JvIM"},"source":["**밑바닥부터 훈련하는 `Embedding` 층을 사용한 모델**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NHbW8s3QJvIM"},"outputs":[],"source":["inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)   #임베딩 레이어\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)   #양방향 LSTM\n","x = layers.Dropout(0.5)(x)   #드롭아웃 레이어\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)   #출력층\n","\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [ keras.callbacks.ModelCheckpoint(\"embeddings_bidir_lstm.keras\",\n","                                    save_best_only=True)]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"embeddings_bidir_lstm.keras\")\n","\n","print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"W32FVDl_JvIM"},"source":["#### 패딩과 마스킹 이해하기"]},{"cell_type":"markdown","metadata":{"id":"M37QJPqIJvIN"},"source":["**마스킹을 활성화한 `Embedding` 층 사용하기**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VS7crehBJvIN"},"outputs":[],"source":["inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = layers.Embedding(\n","    input_dim=max_tokens, output_dim=256,\n","    mask_zero=True)(inputs)   #입력 시퀀스에서 값이 0인 토큰을 마스킹(모델이 해당 토큰을 무시하도록)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [ keras.callbacks.ModelCheckpoint(\"embeddings_bidir_lstm_with_masking.keras\",\n","                                    save_best_only=True) ]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"embeddings_bidir_lstm_with_masking.keras\")\n","print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"]},{"cell_type":"markdown","metadata":{"id":"WT_UyLakJvIN"},"source":["#### 사전 훈련된 단어 임베딩 사용하기"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QST9q6Q1JvIN","outputId":"1c654aa1-9b08-473c-ad16-2a3bb8c5e286","executionInfo":{"status":"ok","timestamp":1689303272433,"user_tz":-540,"elapsed":181441,"user":{"displayName":"김나영","userId":"16985821314079982465"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-07-14 02:51:29--  http://nlp.stanford.edu/data/glove.6B.zip\n","Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n","--2023-07-14 02:51:30--  https://nlp.stanford.edu/data/glove.6B.zip\n","Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n","HTTP request sent, awaiting response... 301 Moved Permanently\n","Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n","--2023-07-14 02:51:30--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n","Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n","Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 862182613 (822M) [application/zip]\n","Saving to: ‘glove.6B.zip’\n","\n","glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 39s  \n","\n","2023-07-14 02:54:09 (5.19 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n","\n"]}],"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip"]},{"cell_type":"markdown","metadata":{"id":"t0aUgmK2JvIN"},"source":["**GloVe 단어 임베딩 파일 파싱하기**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xZwkvoGYJvIN"},"outputs":[],"source":["import numpy as np\n","path_to_glove_file = \"glove.6B.100d.txt\"\n","\n","embeddings_index = {}\n","with open(path_to_glove_file) as f:\n","    for line in f:\n","        word, coefs = line.split(maxsplit=1)\n","        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","        embeddings_index[word] = coefs\n","\n","print(f\"단어 벡터 개수: {len(embeddings_index)}\")"]},{"cell_type":"markdown","metadata":{"id":"hCdliBn7JvIO"},"source":["**GloVe 단어 임베딩 행렬 준비하기**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTEMsvf2JvIO"},"outputs":[],"source":["embedding_dim = 100\n","\n","vocabulary = text_vectorization.get_vocabulary()\n","word_index = dict(zip(vocabulary, range(len(vocabulary))))\n","\n","embedding_matrix = np.zeros((max_tokens, embedding_dim))\n","for word, i in word_index.items():\n","    if i < max_tokens:\n","        embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PmEvrFkYJvIO"},"outputs":[],"source":["embedding_layer = layers.Embedding(\n","    max_tokens,\n","    embedding_dim,\n","    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","    trainable=False,\n","    mask_zero=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"m24hC78pJvIO"},"source":["**사전 훈련된 임베딩을 사용하는 모델**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CAdQbaClJvIO","outputId":"9a539b14-cce9-45a1-f8ac-4255f5965041"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, None)]            0         \n","                                                                 \n"," embedding_3 (Embedding)     (None, None, 100)         2000000   \n","                                                                 \n"," bidirectional_3 (Bidirectio  (None, 64)               34048     \n"," nal)                                                            \n","                                                                 \n"," dropout_3 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 2,034,113\n","Trainable params: 34,113\n","Non-trainable params: 2,000,000\n","_________________________________________________________________\n","Epoch 1/10\n","625/625 [==============================] - 42s 54ms/step - loss: 0.5796 - accuracy: 0.6890 - val_loss: 0.5212 - val_accuracy: 0.7404\n","Epoch 2/10\n","625/625 [==============================] - 31s 49ms/step - loss: 0.4544 - accuracy: 0.7925 - val_loss: 0.4131 - val_accuracy: 0.8040\n","Epoch 3/10\n","625/625 [==============================] - 29s 46ms/step - loss: 0.4012 - accuracy: 0.8217 - val_loss: 0.4275 - val_accuracy: 0.8010\n","Epoch 4/10\n","625/625 [==============================] - 31s 50ms/step - loss: 0.3688 - accuracy: 0.8401 - val_loss: 0.3546 - val_accuracy: 0.8480\n","Epoch 5/10\n","625/625 [==============================] - 31s 50ms/step - loss: 0.3457 - accuracy: 0.8554 - val_loss: 0.3417 - val_accuracy: 0.8486\n","Epoch 6/10\n","625/625 [==============================] - 30s 48ms/step - loss: 0.3238 - accuracy: 0.8653 - val_loss: 0.3416 - val_accuracy: 0.8572\n","Epoch 7/10\n","625/625 [==============================] - 30s 47ms/step - loss: 0.3055 - accuracy: 0.8747 - val_loss: 0.3185 - val_accuracy: 0.8690\n","Epoch 8/10\n","625/625 [==============================] - 28s 45ms/step - loss: 0.2899 - accuracy: 0.8816 - val_loss: 0.3523 - val_accuracy: 0.8602\n","Epoch 9/10\n","625/625 [==============================] - 29s 46ms/step - loss: 0.2752 - accuracy: 0.8874 - val_loss: 0.3116 - val_accuracy: 0.8738\n","Epoch 10/10\n","500/625 [=======================>......] - ETA: 4s - loss: 0.2687 - accuracy: 0.8923"]}],"source":["inputs = keras.Input(shape=(None,), dtype=\"int64\")\n","embedded = embedding_layer(inputs)\n","x = layers.Bidirectional(layers.LSTM(32))(embedded)\n","x = layers.Dropout(0.5)(x)\n","outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n","model = keras.Model(inputs, outputs)\n","model.compile(optimizer=\"rmsprop\",\n","              loss=\"binary_crossentropy\",\n","              metrics=[\"accuracy\"])\n","model.summary()\n","\n","callbacks = [\n","    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n","                                    save_best_only=True)\n","]\n","model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n","model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n","print(f\"테스트 정확도: {model.evaluate(int_test_ds)[1]:.3f}\")"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4","gpuClass":"premium"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"premium"},"nbformat":4,"nbformat_minor":0}