{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f7738cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import t, norm, chi2, chi2_contingency\n",
    "\n",
    "from matplotlib import rc\n",
    "rc('font', family='Malgun Gothic')      #한글 폰트설정\n",
    "plt.rcParams['axes.unicode_minus']=False      #마이너스 부호 출력 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2218b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, scipy, sympy\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests, re, urllib\n",
    "\n",
    "from urllib import robotparser, request\n",
    "from urllib.request import urlopen\n",
    "import chardet\n",
    "\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from time import sleep\n",
    "\n",
    "import os, sys, json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56c6eed",
   "metadata": {},
   "source": [
    "# API (Application Programming Interface)\n",
    "\n",
    "API란 웹 상에서 데이터를 주고받을 수 있도록 미리 정해진 규칙과 프로토콜을 제공하는 인터페이스입니다. 인공지능 개발자는 이러한 API를 통해 데이터를 쉽게 수집하고 활용할 수 있습니다.\n",
    "\n",
    "날씨 정보, 지리 정보, 주가 정보, 소셜 미디어 데이터 등 다양한 데이터를 API를 통해 수집할 수 있습니다. 이렇게 수집한 데이터를 기반으로 인공지능 모델을 학습시키고 예측 모델을 구축할 수 있습니다.\n",
    "\n",
    "API를 사용하면 데이터를 실시간으로 수집하는 것도 가능합니다. 데이터 분석가나 인공지능 개발자는 API를 통해 빠르게 변화하는 데이터를 수집하고, 이를 바탕으로 인공지능 모델을 최신화하고 새로운 트렌드에 대응할 수 있습니다.\n",
    "\n",
    "또한, 데이터를 가공하거나 필요한 부분만 추출할 수도 있습니다. 이를 통해 인공지능 개발자는 필요한 데이터만 추출하고 이를 활용하여 모델을 더욱 효과적으로 구성할 수 있습니다.\n",
    "\n",
    "이러한 API를 활용한 데이터 수집은 파이썬에서도 쉽게 구현할 수 있습니다. \n",
    "requests 라이브러리를 이용하여 API를 호출하고, JSON 또는 XML과 같은 형식으로 데이터를 수집하고 가공할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6144f86",
   "metadata": {},
   "source": [
    "## Naver API\n",
    "\n",
    "https://developers.naver.com/main/\n",
    "\n",
    "\n",
    "\n",
    "네이버 API는 네이버에서 제공하는 다양한 서비스들을 이용할 수 있도록 제공하는 API입니다. 네이버 블로그 검색, 지식인 검색, 뉴스 검색 등의 기능을 API를 통해 사용할 수 있습니다. 이를 이용하여 웹 애플리케이션, 모바일 애플리케이션 등 다양한 서비스를 개발할 수 있습니다.\n",
    "\n",
    "[사용방법]\n",
    "\n",
    "1.\t네이버 개발자 센터에서 애플리케이션을 등록하고 클라이언트 아이디와 클라이언트 시크릿을 발급\n",
    "2.\t애플리케이션에 사용할 네이버 오픈API를 사용 API에서 선택해 추가\n",
    "3.\t로그인 오픈 API 서비스 환경별 상세 정보는 로그인 오픈 API 서비스 환경에서 입력\n",
    "\n",
    "네이버 오픈API는 인증 여부에 따라 로그인 방식 오픈 API와 비로그인 방식 오픈 API로 구분됩니다. 로그인 방식 오픈 API는 '네이버 로그인’의 인증을 받아 접근 토큰(access token)을 획득해야 사용할 수 있는 오픈 API입니다. API를 호출할 때 네이버 로그인 API를 통해 받은 접근 토큰의 값을 전송해야 합니다\n",
    "\n",
    "\n",
    "https://seo.tbwakorea.com/blog/naver-seo-api-searching-data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee4d131",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 네이버 검색 API 제: 블로그 검색 ####\n",
    "\n",
    "# 클라이언트 id와 시크릿 (네이버 디벨로퍼에서 제공)\n",
    "c_id = '7bthh8fHr8j5UpnC7QdK'\n",
    "c_secret = 'mbSLGd_xVC'\n",
    "\n",
    "# 검색어에 대한 url인코딩 수행\n",
    "search = urllib.parse.quote('인공지능')\n",
    "\n",
    "# url변수 = 검색어를 포함한 검색 API의 url이 저장되어 있음\n",
    "url='https://openapi.naver.com/v1/search/blog.json?query=' + search   # JSON 결과\n",
    "url='https://openapi.naver.com/v1/search/blog.xml?query=' + search   # XML 결과\n",
    "\n",
    "# API에 대한 요청 객체(request)를 생성\n",
    "request = urllib.request.Request(url)\n",
    "\n",
    "# 클라이언트 id와 시크릿 값을 요청 헤더에 포함\n",
    "request.add_header('X-Naver-Client-Id', c_id)\n",
    "request.add_header('X-Naver-Client-Secret', c_secret)\n",
    "\n",
    "# 요청 객체 전송 / API에서 반환한 응답(request) 객체 받아옴\n",
    "res = urllib.request.urlopen(request)\n",
    "\n",
    "# HTTP응답 코드 확인\n",
    "rescode = res.getcode()\n",
    "\n",
    "if(rescode==200):\n",
    "    res_body = res.read()\n",
    "    t = res_body.decode('utf-8')    # 문자열 형태로 디코딩\n",
    "    print(t,'\\n\\n')\n",
    "    print('\\n'.join(re.findall('>([^><]+)<', t)))   # 정규표현식으로 정리\n",
    "else:\n",
    "    print('Error Code: ' + str(rescode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e9fbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 검색 API예제는 블로그를 비롯 전문자료까지 호출방법이 동일 (blog 검색만 대표로 예제 확인)\n",
    "# xml 결과\n",
    "client_id = \"7bthh8fHr8j5UpnC7QdK\"\n",
    "client_secret = \"mbSLGd_xVC\"\n",
    "\n",
    "query = urllib.parse.quote(input('검색 질의 : '))   #검색어 입력받음\n",
    "\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.json?query=\" + query # json 결과\n",
    "url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + query #xml 결과\n",
    "\n",
    "request = urllib.request.Request(url)\n",
    "\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "if (rescode==200):\n",
    "    response_body = response.read()\n",
    "    t = response_body.decode('utf-8')    # 문자열 형태로 디코딩\n",
    "#     print(t,'\\n\\n')     #디코딩 된 원본\n",
    "    print('\\n'.join(re.findall('>([^><]+)<', t)))   # 정규표현식으로 정리\n",
    "else:\n",
    "    print(\"Error Code: \" + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd81d200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위와 같은 코드(제이슨 결과 + 딕셔너리에 제목, 링크, 내용, 날짜 저장)\n",
    "import json\n",
    "client_id = \"7bthh8fHr8j5UpnC7QdK\"\n",
    "client_secret = \"mbSLGd_xVC\"\n",
    "\n",
    "query = urllib.parse.quote(input('검색 질의 : '))   #검색어 입력받음\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/blog.json?query=\" + query # json 결과\n",
    "# url = \"https://openapi.naver.com/v1/search/blog.xml?query=\" + query #xml 결과\n",
    "\n",
    "request = urllib.request.Request(url)\n",
    "\n",
    "request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "if (rescode==200):\n",
    "    response_body = response.read()\n",
    "    t = json.loads(response_body.decode('utf-8'))    # 문자열 형태로 디코딩\n",
    "    \n",
    "    items = t['items']   # 제이슨 결과  -> 각 포스트 정보가 딕셔너리 형태로 돼있음\n",
    "    for i in range(0,len(items)):\n",
    "        remove_tag = re.compile('<.*?>')    # 태그 없애기\n",
    "        \n",
    "        title = re.sub(remove_tag, '', items[i]['title'])\n",
    "        link = re.sub(remove_tag, '', items[i]['link'])\n",
    "        description = re.sub(remove_tag, '', items[i]['description'])\n",
    "        postdate = re.sub(remove_tag, '', items[i]['postdate'])\n",
    "        \n",
    "        df.loc[i] = [title, link, description, postdate]\n",
    "\n",
    "else:\n",
    "    print(\"Error Code: \" + rescode)\n",
    "    \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f67e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=response_body.decode('utf-8')\n",
    "df = pd.DataFrame(columns=['title','link','description','postdate'])\n",
    "\n",
    "df.title = re.findall('\"title\":\"([^\"]+)',t)\n",
    "df.link = re.findall('\"link\":\"([^\"]+)',t)\n",
    "df.description = re.findall('\"description\":\"([^\"]+)',t)\n",
    "df.postdate = re.findall('\"postdate\":\"([^\"]+)',t)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f82b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 뉴스 데이터 가져오기 (데이터프레임에 저장)\n",
    "# 100개씩 총 1000개 기사\n",
    "\n",
    "client_id = \"7bthh8fHr8j5UpnC7QdK\"\n",
    "client_secret = \"mbSLGd_xVC\"\n",
    "\n",
    "query = urllib.parse.quote(input('검색 질의 : '))   #검색어 입력받음\n",
    "\n",
    "idx = 0\n",
    "display = 100    # 한 번에 보여줄 검색결과 개수\n",
    "start = 1   # 검색 시작 인덱스\n",
    "end = 1000   # 검색 끝 인덱스\n",
    "sort = 'sim'    # 정확도순 내림차순 정렬 (date: 날짜순 내림차순 정렬)\n",
    "\n",
    "news_df = pd.DataFrame(columns=['Title','Original Link', 'Link','Description', 'Publication Date'])\n",
    "\n",
    "for i in range(start, end, display):\n",
    "    url = 'https://openapi.naver.com/v1/search/news.json?'\n",
    "    url += ('query=' + query)\n",
    "    url += ('&display=' + str(display))\n",
    "    url += ('&start=' + str(start))\n",
    "    url += ('&sort=' + sort)\n",
    "\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "\n",
    "    if (rescode==200):\n",
    "        response_body = response.read()\n",
    "        t = json.loads(response_body.decode('utf-8'))\n",
    "        items = t['items']\n",
    "\n",
    "        for i in range(0,len(items)):   # 각 기사의 정보가 리스트로 들어가 있음\n",
    "            remove_tag = re.compile('<.*?>')   # 태그 제거\n",
    "\n",
    "            title = re.sub(remove_tag, '', items[i]['title'])   # 리스트 안에는 딕셔너리 형태\n",
    "            ori_link = re.sub(remove_tag, '', items[i]['originallink'])\n",
    "            link = re.sub(remove_tag, '', items[i]['link'])\n",
    "            des = re.sub(remove_tag, '', items[i]['description'])\n",
    "            date = re.sub(remove_tag, '', items[i]['pubDate'])\n",
    "\n",
    "            news_df.loc[i] = [title, ori_link, link, des, date]   # 데이터프레임에 삽입\n",
    "\n",
    "    else:\n",
    "        print(\"Error Code: \" + rescode)\n",
    "#     print(news_df)     #매 for문 반복마다 100개씩 기사를 보여줌\n",
    "\n",
    "news_df    #여기서 보는 건 마지막 901~1000번 구간의 100개 기사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0657fa1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f53bb30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99e4007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 뉴스 데이터 가져오기 (데이터프레임에 저장)\n",
    "# 100개씩 총 1000개 기사\n",
    "\n",
    "client_id = \"7bthh8fHr8j5UpnC7QdK\"\n",
    "client_secret = \"mbSLGd_xVC\"\n",
    "\n",
    "query = urllib.parse.quote(input('검색 질의 : '))   #검색어 입력받음\n",
    "\n",
    "idx = 0\n",
    "display = 100    # 한 번에 보여줄 검색결과 개수\n",
    "start = 1   # 검색 시작 인덱스\n",
    "end = 1000   # 검색 끝 인덱스\n",
    "sort = 'sim'    # 정확도순 내림차순 정렬 (date: 날짜순 내림차순 정렬)\n",
    "\n",
    "news_df = pd.DataFrame(columns=['Title','Original Link', 'Link','Description', 'Publication Date'])\n",
    "\n",
    "for i in range(start, end, display):\n",
    "    url = 'https://openapi.naver.com/v1/search/news.json?'\n",
    "    url += ('query=' + query)\n",
    "    url += ('&display=' + str(display))\n",
    "    url += ('&start=' + str(start))\n",
    "    url += ('&sort=' + sort)\n",
    "\n",
    "    request = urllib.request.Request(url)\n",
    "    request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "    request.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    rescode = response.getcode()\n",
    "\n",
    "    if (rescode==200):\n",
    "        response_body = response.read()\n",
    "        t = json.loads(response_body.decode('utf-8'))\n",
    "        items = t['items']\n",
    "\n",
    "        for i in range(0,len(items)):   # 각 기사의 정보가 리스트로 들어가 있음\n",
    "            remove_tag = re.compile('<.*?>')   # 태그 제거\n",
    "\n",
    "            title = re.sub(remove_tag, '', items[i]['title'])   # 리스트 안에는 딕셔너리 형태\n",
    "            ori_link = re.sub(remove_tag, '', items[i]['originallink'])\n",
    "            link = re.sub(remove_tag, '', items[i]['link'])\n",
    "            des = re.sub(remove_tag, '', items[i]['description'])\n",
    "            date = re.sub(remove_tag, '', items[i]['pubDate'])\n",
    "\n",
    "            news_df.loc[i] = [title, ori_link, link, des, date]   # 데이터프레임에 삽입\n",
    "\n",
    "    else:\n",
    "        print(\"Error Code: \" + rescode)\n",
    "#     print(news_df)     #매 for문 반복마다 100개씩 기사를 보여줌\n",
    "\n",
    "news_df    #여기서 보는 건 마지막 901~1000번 구간의 100개 기사"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee521f",
   "metadata": {},
   "source": [
    "#### 과제(1): 지식인 1000개 데이터 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f0e6fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 : 인공지능\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 400: Bad Request",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11292\\769364500.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X-Naver-Client-Id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mc_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_header\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"X-Naver-Client-Secret\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_secret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[0mrescode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m         \u001b[0mopener\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    521\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 523\u001b[1;33m             \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[1;34m(self, request, response)\u001b[0m\n\u001b[0;32m    630\u001b[0m         \u001b[1;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 632\u001b[1;33m             response = self.parent.error(\n\u001b[0m\u001b[0;32m    633\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, proto, *args)\u001b[0m\n\u001b[0;32m    559\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'default'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'http_error_default'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 561\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m \u001b[1;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\urllib\\request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[1;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 641\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mHTTPError\u001b[0m: HTTP Error 400: Bad Request"
     ]
    }
   ],
   "source": [
    "c_id = \"7bthh8fHr8j5UpnC7QdK\"\n",
    "c_secret = \"mbSLGd_xVC\"\n",
    "\n",
    "query = urllib.parse.quote(input('검색어 : '))\n",
    "url='https://openapi.naver.com/v1/search/kin.json?query='+query+'&display=1000'+'&start=1'+'&sort=sim'\n",
    "df = pd.DataFrame(columns=['Title','Link','Description'])\n",
    "\n",
    "\n",
    "url = 'https://openapi.naver.com/v1/search/kin.json?'\n",
    "url += ('query=' + query)\n",
    "url += ('&display=' + '1000')\n",
    "url += ('&start=' + '1')\n",
    "url += ('&sort=' + 'sim')\n",
    "\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header(\"X-Naver-Client-Id\",c_id)\n",
    "request.add_header(\"X-Naver-Client-Secret\", c_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "\n",
    "# req = urllib.request.Request(url)\n",
    "# req.add_header(\"X-Naver-Client-Id\", c_id)\n",
    "# req.add_header(\"X-Naver-Client-Secret\", c_secret)\n",
    "\n",
    "# response = urllib.request.urlopen(req)\n",
    "# code = response.getcode()\n",
    "\n",
    "if rescode==200:\n",
    "    data = response.read()\n",
    "    text = json.loads(data.decode('utf-8'))\n",
    "    items = text['items']\n",
    "    items\n",
    "    \n",
    "#     for i in range(0, len(items)):\n",
    "#         re = re.compile('<.*?>')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3053c785",
   "metadata": {},
   "source": [
    "## OpenWeatherMap 날씨정보\n",
    "\n",
    "\n",
    "https://home.openweathermap.org/users/sign_up\n",
    "\n",
    "[사용방법]\n",
    "\n",
    "1. https://openweathermap.org/api 사이트에서 Current Weather data에 대한 API doc 내용을 파악\n",
    "2.\tOpenWeatherMap 홈페이지에서 회원가입\n",
    "3.\t회원가입 후 API Key를 발급. New Account 등록 후 API Keys라는 탭에서 API Key 확인 가능\n",
    "4.\t발급받은 API Key를 사용하여 API를 호출\n",
    "\n",
    "\n",
    "* 기본적으로 유료 사이트이지만 현재 날씨, 5일까지의 날씨는 무료로 사용할 수 있음(단 1분에 60번만 호출 가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d03cb9",
   "metadata": {},
   "source": [
    "#### 과제(2): 서울, 도쿄, 뉴욕의 날씨 조회를 아래와 같은 형식으로 출력하세요. \n",
    "\n",
    "* text로 가져와서 json으로 변환하여 일차 출력\n",
    "  - cities = [\"Seoul,KR\", \"Tokyo,JP\", \"New York,US\"]\n",
    "  - 가져올 정보 : 도시별 날씨(description), 최저 기온(temp_min), 최고 기온(temp_max), 습도(humidity), 기압(pressure), 풍속(speed)\n",
    "  - 기온 데이터는 켈빈 온도로 되어 있으며 섭씨 온도로 변환해서 출력\n",
    "* json 포멧으로 출력 데이터를 아래와 같이 가독력 있게 출력(소수점 2번째에서 반올림 처리)\n",
    "\n",
    "\n",
    "[출력형식]\n",
    "\n",
    "    도시 = Seoul\n",
    "    | 날씨 = broken clouds\n",
    "    | 최저 기온 = -1.0 2\n",
    "    | 최고 기온 = 2.0 2\n",
    "    | 습도 = 74\n",
    "    | 기압 = 1023\n",
    "    | 풍속 = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0aa973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11dc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
